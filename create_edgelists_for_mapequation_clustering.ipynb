{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os, sys\n",
    "import regex\n",
    "import random\n",
    "from collections import Counter\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_patt = regex.compile(r'RT ')\n",
    "mention_patt = regex.compile(r'@\\w+')\n",
    "hashtag_patt = regex.compile(r'#\\w+')\n",
    "url_patt = regex.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "emoji_patt = regex.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "slash_patt = regex.compile(r'\\/')\n",
    "multispace_patt = regex.compile('\\u0020{2,}')\n",
    "cutoff_patt = regex.compile(r'\\w+…')\n",
    "whitespace_patt = regex.compile(r'\\n{2,}')\n",
    "digit_patt = regex.compile(r'\\d+')\n",
    "ws_patt = regex.compile(r'[\\.\\,\\:\\;\\-\\?\\!\\\"\\'\\(\\)…\\*]',regex.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(line):\n",
    "    no_urls = regex.sub(url_patt,'',line)\n",
    "    no_rt = regex.sub(rt_patt,'',no_urls)\n",
    "    no_slash = regex.sub(slash_patt,' ',no_rt)\n",
    "    no_ats = regex.sub(mention_patt,'',no_slash)\n",
    "    no_tags = regex.sub(hashtag_patt,'',no_ats)\n",
    "    no_emoji = regex.sub(emoji_patt,'',no_tags)\n",
    "    no_multispace = regex.sub(multispace_patt,' ',no_emoji)\n",
    "    no_cutoff = regex.sub(cutoff_patt,'',no_multispace)\n",
    "    no_digits = regex.sub(digit_patt,'',no_cutoff)\n",
    "    nows_line = regex.sub(ws_patt,'',no_digits)\n",
    "    lower_line = nows_line.lower()\n",
    "    clean_text = regex.sub(whitespace_patt,' ',lower_line)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_edgelist_from_userhits(user_hits):\n",
    "    \"\"\"Takes a list of adjectives/adverbs and returns an edgelist e.g.\n",
    "        [('klar', 'fett'),\n",
    "     ('klar', 'klar'),\n",
    "     ('klar', 'absolut'),\n",
    "     ('fett', 'absolut'),\n",
    "     ('klar', 'fett'),\n",
    "     ('klar', 'absolut')]\n",
    "    \"\"\"\n",
    "    user_hits_edgelist = []\n",
    "        \n",
    "    for source in user_hits:\n",
    "        myindex = user_hits.index(source)\n",
    "        newlist = user_hits[:myindex]+user_hits[myindex+1:]   #make a new temp list without the person in it\n",
    "        for item in newlist:\n",
    "            if source != item: # we don't want self-references e.g. (\"god\",\"god\")\n",
    "                mytuple = (source, item)\n",
    "                backtuple = (item, source)\n",
    "                if backtuple not in set(user_hits_edgelist): #remove any reversed duplicates\n",
    "                    user_hits_edgelist.append(mytuple)\n",
    "    return user_hits_edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_edgelist(G, files, graph_name, run):\n",
    "    \n",
    "    G = G\n",
    "    files = files\n",
    "    graph_name = graph_name\n",
    "    \n",
    "    path = '/Volumes/mos_storage/data/totalUsertexts/'\n",
    "    plus_300_lines_files = 1\n",
    "    analyzed_files = 1\n",
    "    \n",
    "    word_count = Counter()\n",
    "    tot_word_count = 0\n",
    "    tot_start = timeit.default_timer()\n",
    "    \n",
    "    for index, fname in enumerate(files):\n",
    "        start_time = timeit.default_timer()\n",
    "        f = open(path + fname)\n",
    "        counter = Counter()\n",
    "        user_hits = []\n",
    "        edgelist = []\n",
    "    \n",
    "        # only analyze large files\n",
    "        if len(f.readlines()) > 300:\n",
    "            plus_300_lines_files += 1\n",
    "            f.close()\n",
    "            original_txt = open(path + fname).read()\n",
    "            clean_txt = normalize_text(original_txt)\n",
    "    \n",
    "            # Open the adjectives/adverbs file again since the iterator is exhausted since before\n",
    "            adjs = open(\"./interesting_adverbs-adjectives\")\n",
    "            \n",
    "            for adj in adjs:\n",
    "                the_string = \"\\\\b\" + adj.rstrip() + \"\\\\b\"\n",
    "                patt = regex.compile(the_string)\n",
    "                match = patt.findall(clean_txt)\n",
    "                user_hits.extend(match)\n",
    "                word_count.update({adj.rstrip():len(match)})\n",
    "            \n",
    "            if len(user_hits) > 0:\n",
    "                tokens = clean_txt.split()\n",
    "            \n",
    "                tot_word_count += len(tokens)\n",
    "            \n",
    "                counter.update(user_hits)\n",
    "            \n",
    "                edgelist = create_edgelist_from_userhits(list(counter.keys()))\n",
    "\n",
    "                for source, target in edgelist:\n",
    "                \n",
    "                    source_degree = counter[source]\n",
    "                    target_degree = counter[target]\n",
    "                    # there can only be as many edges as the lesser count of either source or target\n",
    "                    weight = min(source_degree,target_degree)\n",
    "                \n",
    "                    try:\n",
    "                        if nx.has_path(G,source,target):\n",
    "                            #print(\"Edge exists from {} to {}. Old w {} new w {}\".format(source, target,G[source][target][\"weight\"],G[source][target][\"weight\"] + weight))\n",
    "                            G[source][target][\"weight\"] += weight\n",
    "                \n",
    "                    except Exception as e:\n",
    "                        # make sure nodes exists\n",
    "                        G.add_node(source)\n",
    "                        G.add_node(target)\n",
    "                        G[source][target][\"weight\"] = weight\n",
    "                \n",
    "                    G.add_edge(source, target)\n",
    "                \n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                print(\"After {} - added {} user_hits and {} edges for file {}\".format(elapsed,len(user_hits),len(edgelist),fname), end=\"\\r\")\n",
    "            else:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                print(\"After {} - {} didn't match any adjectives or adverbs\".format(elapsed,fname),flush=True, end=\"\\r\")\n",
    "            \n",
    "        else:\n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            print(\"After {} - {} didn't contain > 300 clean lines\".format(elapsed, fname),flush=True, end=\"\\r\")\n",
    "        \n",
    "        f.close()\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(\"After {} - File no {} of {}\".format(elapsed, index, len(files)),end=\"\\r\")\n",
    "    \n",
    "    for node in G.nodes_iter():\n",
    "        # see http://stackoverflow.com/a/24685791\n",
    "        G.node[node][\"word_count\"] = word_count[node]\n",
    "\n",
    "    tot_elapsed = timeit.default_timer() - tot_start\n",
    "    print(\"After {} run {} - Analyzed {} +300 line files containing {} total clean tokens\".format(tot_elapsed,run, analyzed_files, tot_word_count))\n",
    "    \n",
    "    path = \"/Users/mos/twitterdb/graphs/\"\n",
    "    nx.write_graphml(G,path+graph_name+\".graphml\" )\n",
    "    print(\"Saved graph file {}\".format(path+graph_name+\".graphml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

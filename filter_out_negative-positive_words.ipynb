{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrerar ut positiva och negativa ord enkelt (!) med deep learning \n",
    "## Filosofi och metod\n",
    "Vilka ord och begrepp som uppfattas som eller påverkar mottagaren negativt respektive positivt psykologiskt hör till filosofi snarare än vetenskap. Särskilt eftersom människor har olika värderingar, såsom exempelvis traditionella, modernistiska eller postmoderna, och därför lägger olika tonvikt vid olika aspekter av verkligheten eller lägger in olika betydelse i värdeladdade ord som demokrati, rättvisa, solidaritet etc. \n",
    "\n",
    "En rimlig utgångspunkt kan dock vara att börja i en annan ända, nämligen i förklenande respektive berömmande omdömen om idéer, ting eller andra människor. Det är ett välkänt fenomen att vi människor \"lägger på\" ett språkligt lager som förmedlar positiva och negativa känslor på skeenden. Ett klassiskt exempel inom medievetenskapen är \"frihetskämpe\" respektive \"upprorsmakare\". Ett annat liknande är \"martyr\" respektive \"terrorist\".\n",
    "\n",
    "Fenomenet *name calling* är mycket vanligt och uppfinningsrikedomen är stor till exempel på Twitter där politik  (liksom dess nära släkting *fotboll*) ofta avhandlas i realtid och utan alltför mycket inblandning av det numera förlegade och därför lätt ignorerade *överjaget*. En möjlighet är alltså att leta på Twitter efter ord och begrepp som människor använder för att medvetet eller omedvetet kasta skugga över någon eller någonting respektive att förhärliga och hylla.\n",
    "\n",
    "Det finns i grunden två diametralt motsatta sätt att undersöka språk. Antingen utgår du ifrån regler och fasta hierarkier eller så utgår du från hur människor faktiskt använder språket och låter det flyta och förändras över tid. Att kombinera dem båda är förstås klokast, dock är det en konstart, snarare än en exakt vetenskap, liksom det mesta i livet värt att göra. \n",
    "\n",
    "För många lingvistiska problem har sedan några år tillbaka ordrymder, en i grunden ganska enkel och därför mycket galant, metod för att göra statistiska undersökningar av språk blivit mycket populär. Metoden hör till den senare sorten som alltså utgår från hur människor använder språket och uppdateras löpande vartefter en får fatt i nya språkexempel. Om kidsen plötsligt slutar säga \"fett\" om något som är bra och istället börjar säga t ex \"gött\" så kommer språkmodellen att lära sig det och \"förstå\" att de har en liknande betydelse. Detta beror på att ordrymder i grunden handlar om att datorn läser in vilka ord som förekommer före respektive efter det ord man undersöker. Med tillräckligt stort dataunderlag, i storleksordningen många miljoner exempel, så blir resultaten ofta förvånansvärt bra.\n",
    "\n",
    "Jag har tränat en modell på ett urval av tweets från 2015 från cirka 235 000 konton som i huvudsak skriver på svenska och som efter att ha skrubbats rena från hashtags, url:ar och omnämnanden av kontonamn innehåller minst tre ord. Det lämnade kvar drygt 47 miljoner tweets. Dessa är alltså i huvudsak på svenska, men även en mängd engelska, samt norska, finska och danska. Det gör att modellen visar prov på metodens förmåga att översätta riktigt bra mellan olika språk, en förmåga som åtminstonde för ordnörden respektive den som är intresserad av skalbar AI-teknologi numera visat stort intresse för metoden. \n",
    "\n",
    "I Sverige har vi faktiskt legat långt framme på området akademiskt, vilket i sin tur ledde till att textanalysföretaget Gavagai knoppades av. För den som vill få en djupare förståelse av ordrymder rekommenderar jag varmt att läsa Magnus Sahlgrens avhandling som, trots det relativt avancerade ämnet är ett föredöme i lättlästhet även för en språkteknologisk novis.\n",
    "\n",
    "När du hör talas om ordrymder idag är det sannolikt att du istället hör det engelska namnet *word embeddings* eller rent av *deep learning* eftersom den mest populära metoden som släpptes av Google 2013, kallad word2vec, bygger på en grund variant av deep learning, alltså neurala nätverk. Jag är inte tillräckligt insatt i matematiken bakom de idag ganska många olika metoderna för att skapa ordrymder liknar varandra och skiljer sig åt, men för den intresserade har Magnus Sahlgren även skrivit en lättillgänglig bloggpost som reder ut en del av likheter och skillnader.\n",
    "\n",
    "Tack vare brilljanta och generösa människor inom IT och språkteknologi finns det idag sätt att själv experimentera med ordrymder för att till exempel filtrera ut negativa och positiva ord (och fraser) tillgängliga för människor som endast har mycket grunda kunskaper i hur man använder programkod.\n",
    "\n",
    "Ett populärt och mycket lättillgänligt kodbibliotek som låter en använda Google's algoritm word2vec med hjälp av Python är Gensim. Här följer några exempel på hur jag närmar mig problemet med att identifiera positiva och negativa ord och testa hur word2vec fungerar i Gensim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "import pandas as pd # utmärkt dataanalys-bibliotek så vi enkelt kan spara CSV:er etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jag läser in min färdigtränade algoritm beskriven ovan. Det tog ungefär två dygn att träna upp den på 47 miljoner tweets på min dator. För den specialintresserade har jag beräknat 150 vektorer per ord och bara tagit med ord som förekommer fler än 30 gånger och inte tagit med fler-ordskombinationer som \n",
    "- inte bra\n",
    "- Dagens Nyheter\n",
    "- Det Nya Moderaterna\n",
    "\n",
    "Dessa exempel blir i denna variant av modellen enbart följade ord för sig: \"inte\", \"bra\", \"Dagens\", \"Nyheter\", \"Det\", \"Nya\", \"Moderaterna\" med stora respektive små bokstäver och som alltså måste matchas exakt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"./big_model_150_mincount_30_no_stops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi börjar med att kolla vilka 20 ord som enligt algoritmen mest liknar tre negativa ord en kan använda om andra människor. Bry er inte om att det står *positive* i koden, det betyder bara att vi vill att algoritmen ska leta efter dem. Vi kan även komplettera med *negative* för att säga åt den att lägga en negativ vikt på ord som till exempel har flera betydelser, varav vissa inte ska räknas in ungefär på samma sätt som man kan använda ett minustecken vid Google-sökningar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('idioter', 0.7550226449966431),\n",
       " ('kräk', 0.7507501840591431),\n",
       " ('hycklare', 0.7367528676986694),\n",
       " ('fega', 0.7356211543083191),\n",
       " ('rasist', 0.7228284478187561),\n",
       " ('stolpskott', 0.7205626964569092),\n",
       " ('massmördare', 0.7148877382278442),\n",
       " ('dåre', 0.7092620134353638),\n",
       " ('äckel', 0.7045900225639343),\n",
       " ('pajas', 0.7038259506225586),\n",
       " ('kärring', 0.7000449299812317),\n",
       " ('girig', 0.6987353563308716),\n",
       " ('fegisar', 0.6981837749481201),\n",
       " ('blattar', 0.6931677460670471),\n",
       " ('mördare', 0.6923777461051941),\n",
       " ('zigenare', 0.6921749711036682),\n",
       " ('jävla', 0.6913812756538391),\n",
       " ('patetiska', 0.6883847713470459),\n",
       " ('feg', 0.6847716569900513),\n",
       " ('slödder', 0.6809489130973816)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"idiot\",\"svin\", \"lögnare\"],topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som synes fångar metoden upp ord som både säger något om ursprungsordets betydelse, det vill säga liknande adjektiv som \"skojare\", \"kräk\" respektive \"inkompetenta\" och något om ursprungsordets vanligaste sammanhang, det vill säga partipolitik (brrrr... Arma offentliga samtal...) genom att säga att orden \"sossar\" och \"socialister\" ofta befinner sig *nära*, antingen före eller bakom, dessa adjektiv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om vi istället tittar på positiva omdömen om andra så får vi på samma sätt förslag på närliggande ord statistiskt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kämpe', 0.7751948237419128),\n",
       " ('inspirationskälla', 0.722464919090271),\n",
       " ('stjärna', 0.6930128931999207),\n",
       " ('pionjär', 0.6772516965866089),\n",
       " ('krigare', 0.6767622232437134),\n",
       " ('eldsjäl', 0.6764605045318604),\n",
       " ('räddning', 0.665830135345459),\n",
       " ('superhjälte', 0.6634135842323303),\n",
       " ('inspiratör', 0.6594758033752441),\n",
       " ('berättare', 0.6585543155670166),\n",
       " ('hyllning', 0.6584010720252991),\n",
       " ('glädjespridare', 0.6553477048873901),\n",
       " ('dröm', 0.6531526446342468),\n",
       " ('vardagshjälte', 0.6528267860412598),\n",
       " ('världsstjärna', 0.6490020751953125),\n",
       " ('solskenshistoria', 0.6482461094856262),\n",
       " ('ikon', 0.6476023197174072),\n",
       " ('legendar', 0.6445429921150208),\n",
       " ('kärleksförklaring', 0.6341878771781921),\n",
       " ('hedersman', 0.6284327507019043)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"förebild\",\"hjälte\",\"hjältinna\"],topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här ser vi igen en blandning av likhet i *betydelse* såsom i \"stjärna\", \"legendar\", \"förebild\", men även dess *sammanhang* såsom i \"autograf\", \"pappa\" och \"rockstjärna\". En kan fråga sig varför ord som \"groda\" respektive \"galning\" också ofta befinner sig nära dessa adjektiv. Har du nån idé?\n",
    "\n",
    "Jag tycker för övrigt att det är intressant ur ett genusperspektiv att detta hyfsat slumpmässiga och relativt (enkätundersökningar) stora sample av människor förknippar dessa positiva omdömen oftare med maskulinitet, även om \"hjältinna\" är med som det sista ordet. Det säger dels någonting om kulturella värderingar på Twitter och dels någonting om att den här metoden på ett utmärkt sätt lånar sig för storskaliga kulturstuider av till exempel vardagsspråk i sociala medier. \n",
    "\n",
    "Här är fil med råa ord som den som är intresserad av positivt och negativt laddade ord. Kom ihåg att algoritmen endast svarar på vilka ord som statistiskt sett befinner sig *nära* varanda så du inte förleds att tro att den faktiskt förstår ordens betydelse och kan skilja mellan positivt och negativt. Om du läser igenom dem och gör din egen bedömning för vart och ett av orden så kan du enkelt placera dem i rätt kolumn och till exempel dela upp dem i ordform, ämnesområde eller på annat sätt göra dem mer användbara. Fördelen med just de här orden är att de är de statistiskt mest förekommande baserat på ett stort och rikt urval av ord, så även med enkla ordräkningsmetoder torde de ge många träffar på vardagsspråk i sociala medier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg = model.most_similar(positive=[\"idiot\",\"svin\", \"lögnare\"],topn=1000)\n",
    "pos = model.most_similar(positive=[\"förebild\",\"hjälte\",\"hjältinna\"],topn=1000)\n",
    "combined = {\"pos\":pos,\"neg\":neg}\n",
    "df = pd.DataFrame(combined)\n",
    "df.to_csv(\"./pos-neg.csv\", header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om du sitter på egna funderingar eller idéer i hur du skulle vilja tillämpa den här eller liknande metoder tveka inte att prova på Python för textanalys och höra av dig till mig om du vill ha hjälp på något sätt där du tror att jag är bättre än Google.com. \n",
    "\n",
    "[http://www.twitter.com/mattiasostmar](@mattiasostmar)\n",
    "[mailto:mattiasostmar@gmail.com](mattiasostmar@gmail)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
